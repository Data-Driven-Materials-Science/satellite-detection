Registered Datasets: ['satellite_Train', 'satellite_Val']
ddict info:
	path: ../SALAS_Rep/../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S04_03_SE1_1000X53.png
	num_instances: 40
ddict info:
	path: ../SALAS_Rep/../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/HP743_10S_500x.png
	num_instances: 32
ddict info:
	path: ../SALAS_Rep/../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/HP743_11S_500x.png
	num_instances: 25
ddict info:
	path: ../SALAS_Rep/../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S08_03_SE1_1000X03.png
	num_instances: 88
ddict info:
	path: ../SALAS_Rep/../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S06_01_SE1_500X67.png
	num_instances: 54
ddict info:
	path: ../SALAS_Rep/../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S04_02_SE1_1000X50.png
	num_instances: 45
ddict info:
	path: ../SALAS_Rep/../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S03_03_SE1_1250X41.png
	num_instances: 111
ddict info:
	path: ../SALAS_Rep/../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S02_01_SE1_300X14.png
	num_instances: 98
ddict info:
	path: ../SALAS_Rep/../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/HP743_2S_250x.png
	num_instances: 49
Weights not found, weights will be downloaded from source: https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl
[32m[11/11 18:08:20 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[32m[11/11 18:08:20 d2.data.build]: [0mRemoved 0 images with no usable annotations. 20 images left.
[32m[11/11 18:08:20 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
| satellite  | 1412         |
|            |              |[0m
[32m[11/11 18:08:20 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[11/11 18:08:20 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[11/11 18:08:20 d2.data.common]: [0mSerializing 20 elements to byte tensors and concatenating them all ...
[32m[11/11 18:08:20 d2.data.common]: [0mSerialized dataset takes 0.46 MiB
[5m[31mWARNING[0m [32m[11/11 18:08:20 d2.solver.build]: [0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[32m[11/11 18:08:20 d2.checkpoint.c2_model_loading]: [0mRenaming Caffe2 weights ......
[32m[11/11 18:08:21 d2.checkpoint.c2_model_loading]: [0mFollowing weights matched with submodule backbone.bottom_up:
| Names in Model    | Names in Checkpoint      | Shapes                                          |
|:------------------|:-------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*           | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                  | (64, 3, 7, 7)                                   |
[32m[11/11 18:08:21 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[11/11 18:08:23 d2.utils.events]: [0m eta: 0:17:02  iter: 19  total_loss: 3.369  loss_cls: 0.5827  loss_box_reg: 0.02647  loss_mask: 0.6771  loss_rpn_cls: 0.7012  loss_rpn_loc: 1.306  time: 0.1011  data_time: 0.0147  lr: 0.00019981  max_mem: 1774M
[32m[11/11 18:08:25 d2.utils.events]: [0m eta: 0:16:40  iter: 39  total_loss: 2.309  loss_cls: 0.309  loss_box_reg: 0.02843  loss_mask: 0.6033  loss_rpn_cls: 0.6122  loss_rpn_loc: 0.7721  time: 0.1000  data_time: 0.0015  lr: 0.00039961  max_mem: 1799M
[32m[11/11 18:08:27 d2.utils.events]: [0m eta: 0:16:37  iter: 59  total_loss: 2.061  loss_cls: 0.1941  loss_box_reg: 0.06721  loss_mask: 0.5841  loss_rpn_cls: 0.5919  loss_rpn_loc: 0.6556  time: 0.0999  data_time: 0.0016  lr: 0.00059941  max_mem: 1799M
[32m[11/11 18:08:29 d2.utils.events]: [0m eta: 0:16:53  iter: 79  total_loss: 2.047  loss_cls: 0.2199  loss_box_reg: 0.173  loss_mask: 0.5594  loss_rpn_cls: 0.5387  loss_rpn_loc: 0.5412  time: 0.1015  data_time: 0.0017  lr: 0.00079921  max_mem: 1799M
[32m[11/11 18:08:31 d2.utils.events]: [0m eta: 0:16:58  iter: 99  total_loss: 2.338  loss_cls: 0.308  loss_box_reg: 0.3981  loss_mask: 0.5433  loss_rpn_cls: 0.5142  loss_rpn_loc: 0.5351  time: 0.1023  data_time: 0.0016  lr: 0.00099901  max_mem: 1799M
[32m[11/11 18:08:33 d2.utils.events]: [0m eta: 0:16:55  iter: 119  total_loss: 2.164  loss_cls: 0.306  loss_box_reg: 0.4076  loss_mask: 0.5272  loss_rpn_cls: 0.4279  loss_rpn_loc: 0.5103  time: 0.1029  data_time: 0.0018  lr: 0.0011988  max_mem: 1799M
[32m[11/11 18:08:36 d2.utils.events]: [0m eta: 0:16:55  iter: 139  total_loss: 2.012  loss_cls: 0.248  loss_box_reg: 0.3779  loss_mask: 0.4991  loss_rpn_cls: 0.3718  loss_rpn_loc: 0.603  time: 0.1032  data_time: 0.0018  lr: 0.0013986  max_mem: 1824M
[32m[11/11 18:08:38 d2.utils.events]: [0m eta: 0:17:00  iter: 159  total_loss: 1.908  loss_cls: 0.2442  loss_box_reg: 0.3896  loss_mask: 0.4561  loss_rpn_cls: 0.3497  loss_rpn_loc: 0.4974  time: 0.1049  data_time: 0.0017  lr: 0.0015984  max_mem: 1824M
[32m[11/11 18:08:40 d2.utils.events]: [0m eta: 0:17:07  iter: 179  total_loss: 1.926  loss_cls: 0.2856  loss_box_reg: 0.4872  loss_mask: 0.4441  loss_rpn_cls: 0.3051  loss_rpn_loc: 0.4749  time: 0.1056  data_time: 0.0019  lr: 0.0017982  max_mem: 1824M
[32m[11/11 18:08:42 d2.utils.events]: [0m eta: 0:17:08  iter: 199  total_loss: 1.905  loss_cls: 0.2598  loss_box_reg: 0.4647  loss_mask: 0.3924  loss_rpn_cls: 0.305  loss_rpn_loc: 0.4574  time: 0.1062  data_time: 0.0018  lr: 0.001998  max_mem: 1824M
[32m[11/11 18:08:44 d2.utils.events]: [0m eta: 0:17:06  iter: 219  total_loss: 1.848  loss_cls: 0.2913  loss_box_reg: 0.42  loss_mask: 0.3875  loss_rpn_cls: 0.3011  loss_rpn_loc: 0.4621  time: 0.1063  data_time: 0.0016  lr: 0.0021978  max_mem: 1824M
[32m[11/11 18:08:47 d2.utils.events]: [0m eta: 0:17:04  iter: 239  total_loss: 1.771  loss_cls: 0.2571  loss_box_reg: 0.3958  loss_mask: 0.3739  loss_rpn_cls: 0.2771  loss_rpn_loc: 0.4709  time: 0.1064  data_time: 0.0016  lr: 0.0023976  max_mem: 1824M
[32m[11/11 18:08:49 d2.utils.events]: [0m eta: 0:17:04  iter: 259  total_loss: 1.803  loss_cls: 0.2638  loss_box_reg: 0.4315  loss_mask: 0.3529  loss_rpn_cls: 0.3009  loss_rpn_loc: 0.4881  time: 0.1065  data_time: 0.0016  lr: 0.0025974  max_mem: 1824M
[32m[11/11 18:08:51 d2.utils.events]: [0m eta: 0:17:03  iter: 279  total_loss: 1.766  loss_cls: 0.2682  loss_box_reg: 0.5025  loss_mask: 0.3416  loss_rpn_cls: 0.2088  loss_rpn_loc: 0.4505  time: 0.1068  data_time: 0.0017  lr: 0.0027972  max_mem: 1824M
[32m[11/11 18:08:53 d2.utils.events]: [0m eta: 0:17:05  iter: 299  total_loss: 1.74  loss_cls: 0.2574  loss_box_reg: 0.4499  loss_mask: 0.33  loss_rpn_cls: 0.2244  loss_rpn_loc: 0.459  time: 0.1072  data_time: 0.0016  lr: 0.002997  max_mem: 1824M
[32m[11/11 18:08:56 d2.utils.events]: [0m eta: 0:17:11  iter: 319  total_loss: 1.743  loss_cls: 0.2876  loss_box_reg: 0.4775  loss_mask: 0.3282  loss_rpn_cls: 0.2238  loss_rpn_loc: 0.4378  time: 0.1075  data_time: 0.0015  lr: 0.0031968  max_mem: 1824M
[32m[11/11 18:08:58 d2.utils.events]: [0m eta: 0:17:10  iter: 339  total_loss: 1.715  loss_cls: 0.2675  loss_box_reg: 0.509  loss_mask: 0.3212  loss_rpn_cls: 0.2188  loss_rpn_loc: 0.4066  time: 0.1077  data_time: 0.0015  lr: 0.0033966  max_mem: 1824M
[32m[11/11 18:09:00 d2.utils.events]: [0m eta: 0:17:09  iter: 359  total_loss: 1.705  loss_cls: 0.2292  loss_box_reg: 0.5032  loss_mask: 0.3063  loss_rpn_cls: 0.2303  loss_rpn_loc: 0.4172  time: 0.1079  data_time: 0.0016  lr: 0.0035964  max_mem: 1824M
[32m[11/11 18:09:02 d2.utils.events]: [0m eta: 0:17:12  iter: 379  total_loss: 1.649  loss_cls: 0.2397  loss_box_reg: 0.49  loss_mask: 0.2962  loss_rpn_cls: 0.1882  loss_rpn_loc: 0.4331  time: 0.1082  data_time: 0.0015  lr: 0.0037962  max_mem: 1824M
[32m[11/11 18:09:05 d2.utils.events]: [0m eta: 0:17:12  iter: 399  total_loss: 1.69  loss_cls: 0.2648  loss_box_reg: 0.4249  loss_mask: 0.3255  loss_rpn_cls: 0.2252  loss_rpn_loc: 0.4278  time: 0.1082  data_time: 0.0015  lr: 0.003996  max_mem: 1824M
[32m[11/11 18:09:07 d2.utils.events]: [0m eta: 0:17:12  iter: 419  total_loss: 1.575  loss_cls: 0.2507  loss_box_reg: 0.4799  loss_mask: 0.2977  loss_rpn_cls: 0.1953  loss_rpn_loc: 0.3851  time: 0.1084  data_time: 0.0015  lr: 0.0041958  max_mem: 1824M
[32m[11/11 18:09:09 d2.utils.events]: [0m eta: 0:17:13  iter: 439  total_loss: 1.602  loss_cls: 0.26  loss_box_reg: 0.5039  loss_mask: 0.2853  loss_rpn_cls: 0.1972  loss_rpn_loc: 0.4039  time: 0.1085  data_time: 0.0017  lr: 0.0043956  max_mem: 1824M
[32m[11/11 18:09:11 d2.utils.events]: [0m eta: 0:17:09  iter: 459  total_loss: 1.614  loss_cls: 0.2299  loss_box_reg: 0.4719  loss_mask: 0.3001  loss_rpn_cls: 0.2271  loss_rpn_loc: 0.3869  time: 0.1085  data_time: 0.0016  lr: 0.0045954  max_mem: 1824M
[32m[11/11 18:09:14 d2.utils.events]: [0m eta: 0:17:10  iter: 479  total_loss: 1.696  loss_cls: 0.2559  loss_box_reg: 0.4778  loss_mask: 0.2821  loss_rpn_cls: 0.2336  loss_rpn_loc: 0.4052  time: 0.1086  data_time: 0.0016  lr: 0.0047952  max_mem: 1824M
[32m[11/11 18:09:16 d2.utils.events]: [0m eta: 0:17:08  iter: 499  total_loss: 1.614  loss_cls: 0.2714  loss_box_reg: 0.4921  loss_mask: 0.2796  loss_rpn_cls: 0.1871  loss_rpn_loc: 0.4223  time: 0.1087  data_time: 0.0016  lr: 0.004995  max_mem: 1824M
[32m[11/11 18:09:18 d2.utils.events]: [0m eta: 0:17:06  iter: 519  total_loss: 1.701  loss_cls: 0.2298  loss_box_reg: 0.3986  loss_mask: 0.2955  loss_rpn_cls: 0.2921  loss_rpn_loc: 0.4858  time: 0.1087  data_time: 0.0016  lr: 0.0051948  max_mem: 1824M
[32m[11/11 18:09:20 d2.utils.events]: [0m eta: 0:17:03  iter: 539  total_loss: 1.655  loss_cls: 0.2333  loss_box_reg: 0.3936  loss_mask: 0.2849  loss_rpn_cls: 0.232  loss_rpn_loc: 0.4755  time: 0.1087  data_time: 0.0015  lr: 0.0053946  max_mem: 1824M
[32m[11/11 18:09:22 d2.utils.events]: [0m eta: 0:17:03  iter: 559  total_loss: 1.714  loss_cls: 0.2372  loss_box_reg: 0.4279  loss_mask: 0.2954  loss_rpn_cls: 0.2157  loss_rpn_loc: 0.4998  time: 0.1088  data_time: 0.0016  lr: 0.0055944  max_mem: 1824M
[32m[11/11 18:09:25 d2.utils.events]: [0m eta: 0:17:00  iter: 579  total_loss: 1.595  loss_cls: 0.254  loss_box_reg: 0.4807  loss_mask: 0.2662  loss_rpn_cls: 0.1909  loss_rpn_loc: 0.3898  time: 0.1088  data_time: 0.0015  lr: 0.0057942  max_mem: 1824M
[32m[11/11 18:09:27 d2.utils.events]: [0m eta: 0:17:00  iter: 599  total_loss: 1.541  loss_cls: 0.2524  loss_box_reg: 0.4631  loss_mask: 0.2618  loss_rpn_cls: 0.1915  loss_rpn_loc: 0.3946  time: 0.1089  data_time: 0.0016  lr: 0.005994  max_mem: 1824M
[32m[11/11 18:09:29 d2.utils.events]: [0m eta: 0:16:59  iter: 619  total_loss: 1.6  loss_cls: 0.272  loss_box_reg: 0.494  loss_mask: 0.2717  loss_rpn_cls: 0.1828  loss_rpn_loc: 0.3994  time: 0.1090  data_time: 0.0016  lr: 0.0061938  max_mem: 1824M
[32m[11/11 18:09:31 d2.utils.events]: [0m eta: 0:16:57  iter: 639  total_loss: 1.616  loss_cls: 0.2536  loss_box_reg: 0.4902  loss_mask: 0.257  loss_rpn_cls: 0.1995  loss_rpn_loc: 0.4092  time: 0.1091  data_time: 0.0016  lr: 0.0063936  max_mem: 1824M
[32m[11/11 18:09:34 d2.utils.events]: [0m eta: 0:16:56  iter: 659  total_loss: 1.606  loss_cls: 0.2445  loss_box_reg: 0.5191  loss_mask: 0.2715  loss_rpn_cls: 0.1482  loss_rpn_loc: 0.3978  time: 0.1091  data_time: 0.0016  lr: 0.0065934  max_mem: 1824M
[32m[11/11 18:09:36 d2.utils.events]: [0m eta: 0:16:56  iter: 679  total_loss: 1.606  loss_cls: 0.2717  loss_box_reg: 0.495  loss_mask: 0.2668  loss_rpn_cls: 0.2007  loss_rpn_loc: 0.3851  time: 0.1093  data_time: 0.0016  lr: 0.0067932  max_mem: 1824M
[32m[11/11 18:09:38 d2.utils.events]: [0m eta: 0:16:56  iter: 699  total_loss: 1.583  loss_cls: 0.2518  loss_box_reg: 0.5296  loss_mask: 0.2611  loss_rpn_cls: 0.1703  loss_rpn_loc: 0.3739  time: 0.1093  data_time: 0.0015  lr: 0.006993  max_mem: 1824M
[32m[11/11 18:09:40 d2.utils.events]: [0m eta: 0:16:54  iter: 719  total_loss: 1.595  loss_cls: 0.2396  loss_box_reg: 0.4863  loss_mask: 0.2986  loss_rpn_cls: 0.2011  loss_rpn_loc: 0.4274  time: 0.1094  data_time: 0.0016  lr: 0.0071928  max_mem: 1824M
[32m[11/11 18:09:43 d2.utils.events]: [0m eta: 0:16:53  iter: 739  total_loss: 1.609  loss_cls: 0.2486  loss_box_reg: 0.4538  loss_mask: 0.2613  loss_rpn_cls: 0.1976  loss_rpn_loc: 0.4188  time: 0.1094  data_time: 0.0016  lr: 0.0073926  max_mem: 1824M
