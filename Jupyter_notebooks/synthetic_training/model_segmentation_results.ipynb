{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020 Ryan Cohn and Elizabeth Holm. All rights reserved. <br />\n",
    "Licensed under the MIT License (see LICENSE for details) <br />\n",
    "Written by Ryan Cohn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Segmentation of Powder Particles and Satellites\n",
    "\n",
    "This example will take you through the process of training a model to segment powder particles and visualizing the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regular module imports\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import skimage.io\n",
    "import sys\n",
    "\n",
    "## detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import (\n",
    "    DatasetCatalog,\n",
    "    MetadataCatalog,\n",
    ")\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "## ampis\n",
    "root = '../../../'\n",
    "ocean_images = root + '../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/'\n",
    "ocean_syn = root + '../../../../ocean/projects/dmr200021p/sprice/synthetic_training/'\n",
    "sys.path.append(root)\n",
    "\n",
    "\n",
    "from ampis import data_utils, visualize, export_anno\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Data\n",
    "\n",
    "The [VGG Image Annotator](http://www.robots.ox.ac.uk/~vgg/software/via/) was used to generate particle labels. The a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "The process for training models for powder particles and satellites is identical. Enter the corresponding value depending on which model you want to train. <br />\n",
    "The paths to individual images, and all annotation data are stored in the JSON files generated by the VGG image annotator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"json_path_train = Path('training_sets', f'{EXPERIMENT_NAME}_auto_training_v1.7.json')  # path to training data\\njson_path_val = Path('training_sets',f'{EXPERIMENT_NAME}_auto_validation_v1.1.json')  # path to training data\\nassert json_path_train.is_file(), 'training file not found!'\\nassert json_path_val.is_file(), 'validation file not found!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENT_NAME = 'satellite' # can be 'particle' or 'satellite'\n",
    "EXPERIMENT_NUM = '4'\n",
    "'''json_path_train = Path('training_sets', f'{EXPERIMENT_NAME}_auto_training_v1.7.json')  # path to training data\n",
    "json_path_val = Path('training_sets',f'{EXPERIMENT_NAME}_auto_validation_v1.1.json')  # path to training data\n",
    "assert json_path_train.is_file(), 'training file not found!'\n",
    "assert json_path_val.is_file(), 'validation file not found!'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration\n",
    "Detectron2 requires that datasets be registered for later use.\n",
    "Registration stores the name of the dataset and a function that can be used to retrieve the image paths and labels in a format that the model can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "This is where we specify the directory where the outputs are saved, various hyperparameters for the model, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights not found, weights will be downloaded from source: https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_final.pth\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg() # initialize cfg object\n",
    "cfg.merge_from_file(model_zoo.get_config_file('COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'))  # load default parameters for Mask R-CNN\n",
    "cfg.INPUT.MASK_FORMAT = 'polygon'  # masks generated in VGG image annotator are polygons\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1 # number of images per batch (across all machines)\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 1000  # number of iterations after which to save model checkpoints\n",
    "cfg.MODEL.DEVICE='cuda'  # 'cpu' to force model to run on cpu, 'cuda' if you have a compatible gpu\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # Since we are training separate models for particles and satellites there is only one class output\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 400 if EXPERIMENT_NAME == 'particle' else 250  # maximum number of instances that can be detected in an image (this is fixed in mask r-cnn)\n",
    "cfg.SOLVER.MAX_ITER = 5000  # maximum number of iterations to run during training\n",
    "                            # Increasing this may improve the training results, but will take longer to run (especially without a gpu!)\n",
    "\n",
    "weights_path = Path(str(root),'examples','powder','satellite_output_auto','model_final_W3.6.pth')\n",
    "if weights_path.is_file():\n",
    "    print('Using locally stored weights: {}'.format(weights_path))\n",
    "else:\n",
    "    weights_path = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    print('Weights not found, weights will be downloaded from source: {}'.format(weights_path))\n",
    "cfg.MODEL.WEIGHTs = str(weights_path)\n",
    "cfg.OUTPUT_DIR = str(Path(ocean_syn, 'Models', f'Synthetic_training_a1.{EXPERIMENT_NUM}'))\n",
    "# make the output directory\n",
    "model_checkpoints = sorted(Path(cfg.OUTPUT_DIR).glob('*.pth'))  # paths to weights saved druing training\n",
    "cfg.MODEL.WEIGHTS = str(model_checkpoints[-1])  # use the last model checkpoint saved during training. If you want to see the performance of other checkpoints you can select a different index from model_checkpoints.\n",
    "print(str(model_checkpoints[-1]))\n",
    "predictor = DefaultPredictor(cfg)  # create predictor object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions on new images is simple.\n",
    "We will load a new image (not included in either dataset) and generate predictions.\n",
    "Note that we do not have labels for this image and do not need to register it to a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3c6cf7a12842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Auto_annotate_images'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'S01_02_SE1_300X06.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m visualize.display_ddicts(ddict=outs,  # predictions to display\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;31m# whether the model expects BGR inputs or RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0moriginal_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "img_path = Path('Auto_annotate_images','S01_02_SE1_300X06.png')\n",
    "img = cv2.imread(str(img_path))\n",
    "outs = predictor(img)\n",
    "data_utils.format_outputs(img_path, dataset='test', pred=outs)\n",
    "visualize.display_ddicts(ddict=outs,  # predictions to display\n",
    "                                 outpath=None, dataset='Test',  # don't save figure\n",
    "                                 gt=False,  # specifies format as model predictions\n",
    "                                img_path=img_path)  # path to image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate predictions on all of the images in the training and validation sets, and save the results for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n"
     ]
    }
   ],
   "source": [
    "path = ocean_syn + 'images/a-lognormal-loc0.1-shape0.5/renders'\n",
    "results = []  \n",
    "t = ['particles1', 'particles10','particles11']\n",
    "for f in t:\n",
    "    print(\"Annotating: \" + f)\n",
    "    img_path = Path(path, f +'.png')\n",
    "    img = cv2.imread(str(img_path))\n",
    "    outs = predictor(img)\n",
    "    results.append(data_utils.format_outputs(img_path, 'Validation', outs))\n",
    "    #visualize.display_ddicts(ddict=outs,  # predictions to display\n",
    "                                 #outpath=None, dataset='Test',  # don't save fi$\n",
    "                                 #gt=False,  # specifies format as model predict$\n",
    "                                 #img_path=img_path, # path to image\n",
    "                                 #suppress_labels=True, #hides class images\n",
    "                                 #summary=True)  #hides the end print statement        \n",
    "# save to disk\n",
    "with open(Path('Model_Performances', f'{EXPERIMENT_NAME}-model_a1.{EXPERIMENT_NUM}.pickle'), 'wb') as f:\n",
    "    pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['999', '1999', '2999', '3999', '4999', '5999', '6999', '7999', '8999', '9999', '10999', '11999', '12999', '13999', '14999']\n"
     ]
    }
   ],
   "source": [
    "path = ocean_syn + 'images/a-lognormal-loc0.1-shape0.5/renders'\n",
    "results = []  \n",
    "t = ['particles1', 'particles10','particles11']\n",
    "extension = []\n",
    "for i in range(1, 16):\n",
    "    extension.append(str(i * 1000 - 1))\n",
    "print(extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0000999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0001999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0002999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0003999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0004999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0005999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0006999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0007999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0008999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0009999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0010999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0011999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0012999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0013999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n",
      "../../../../../../../ocean/projects/dmr200021p/sprice/synthetic_training/Models/Synthetic_training_a1.4/model_0014999.pth\n",
      "Annotating: particles1\n",
      "Annotating: particles10\n",
      "Annotating: particles11\n"
     ]
    }
   ],
   "source": [
    "path = ocean_syn + 'images/a-lognormal-loc0.1-shape0.5/renders'\n",
    "results = []  \n",
    "t = ['particles1', 'particles10','particles11']\n",
    "count = 0\n",
    "extension = ['999', '1999', '2999', '3999', '4999', '5999', '6999', '7999', '8999', '9999', '10999', '11999', '12999', '13999', '14999']\n",
    "for i in range(0, len(model_checkpoints) - 1):\n",
    "    cfg.MODEL.WEIGHTS = str(model_checkpoints[i])  # use the last model checkpoint saved during training. If you want to see the performance of other checkpoints you can select a different index from model_checkpoints.\n",
    "    print(str(model_checkpoints[i]))\n",
    "    predictor = DefaultPredictor(cfg)  # create predictor object\n",
    "    for f in t:\n",
    "        print(\"Annotating: \" + f)\n",
    "        img_path = Path(path, f +'.png')\n",
    "        img = cv2.imread(str(img_path))\n",
    "        outs = predictor(img)\n",
    "        results.append(data_utils.format_outputs(img_path, 'Validation', outs))\n",
    "        #visualize.display_ddicts(ddict=outs,  # predictions to display\n",
    "                                 #outpath=None, dataset='Test',  # don't save fi$\n",
    "                                 #gt=False,  # specifies format as model predict$\n",
    "                                 #img_path=img_path, # path to image\n",
    "                                 #suppress_labels=True, #hides class images\n",
    "                                 #summary=True)  #hides the end print statement        \n",
    "    \n",
    "    # save to disk\n",
    "    with open(Path('Model_Performances', f'{EXPERIMENT_NAME}-model_a1.{str(EXPERIMENT_NUM)}_section_{extension[i]}.pickle'), 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0b8c658f9da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Auto_annotate_images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'250x'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'500x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimg_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "files = os.listdir('Auto_annotate_images')\n",
    "img_names = []\n",
    "for f in files:\n",
    "    if f.split('.')[0] != '250x' and f.split('.')[0] != '500x':\n",
    "        img_names.append(f.split('.')[0])\n",
    "        \n",
    "results = []\n",
    "for f in img_names:\n",
    "    print(\"Annotating: \" + f)\n",
    "    img_path = Path('Auto_annotate_images', f +'.png')\n",
    "    img = cv2.imread(str(img_path))\n",
    "    outs = predictor(img)\n",
    "    data_utils.format_outputs(img_path, dataset='test', pred=outs)\n",
    "    visualize.display_ddicts(ddict=outs,  # predictions to display\n",
    "                                 outpath=None, dataset='Test',  # don't save fi$\n",
    "                                 gt=False,  # specifies format as model predict$\n",
    "                                 img_path=img_path, # path to image\n",
    "                                 suppress_labels=True, #hides class images\n",
    "                                 summary=True)  #hides the end print statement\n",
    "\n",
    "# save to disk\n",
    "with open(Path('Stage_results',f'{EXPERIMENT_NAME}-base_stage.pickle'), 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
