Registered Datasets: ['satellite_Train', 'satellite_Val']
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S03_01_SE1_1250X28.png
	num_instances: 123
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/HP743_12S_500x.png
	num_instances: 31
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/HP743_2S_500x.png
	num_instances: 47
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S08_03_SE1_1000X03.png
	num_instances: 88
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S06_01_SE1_500X67.png
	num_instances: 54
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S04_02_SE1_1000X50.png
	num_instances: 45
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S03_03_SE1_1250X41.png
	num_instances: 111
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S02_01_SE1_300X14.png
	num_instances: 98
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/HP743_2S_250x.png
	num_instances: 49
Weights not found, weights will be downloaded from source: https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl
[32m[10/25 21:28:38 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[32m[10/25 21:28:38 d2.data.build]: [0mRemoved 0 images with no usable annotations. 20 images left.
[32m[10/25 21:28:38 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
| satellite  | 1412         |
|            |              |[0m
[32m[10/25 21:28:38 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[10/25 21:28:38 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[10/25 21:28:38 d2.data.common]: [0mSerializing 20 elements to byte tensors and concatenating them all ...
[32m[10/25 21:28:38 d2.data.common]: [0mSerialized dataset takes 0.46 MiB
[5m[31mWARNING[0m [32m[10/25 21:28:38 d2.solver.build]: [0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[32m[10/25 21:28:39 d2.checkpoint.c2_model_loading]: [0mRenaming Caffe2 weights ......
[32m[10/25 21:28:39 d2.checkpoint.c2_model_loading]: [0mFollowing weights matched with submodule backbone.bottom_up:
| Names in Model    | Names in Checkpoint      | Shapes                                          |
|:------------------|:-------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*           | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                  | (64, 3, 7, 7)                                   |
[32m[10/25 21:28:39 d2.engine.train_loop]: [0mStarting training from iteration 0
/jet/home/sprice/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[32m[10/25 21:28:43 d2.utils.events]: [0m eta: 0:33:36  iter: 19  total_loss: 2.783  loss_cls: 0.448  loss_box_reg: 0.03041  loss_mask: 0.6494  loss_rpn_cls: 0.639  loss_rpn_loc: 1.026  time: 0.1435  data_time: 0.0974  lr: 0.00059943  max_mem: 1590M
[32m[10/25 21:28:46 d2.utils.events]: [0m eta: 0:33:46  iter: 39  total_loss: 2.146  loss_cls: 0.2568  loss_box_reg: 0.09244  loss_mask: 0.5927  loss_rpn_cls: 0.5605  loss_rpn_loc: 0.6435  time: 0.1404  data_time: 0.0145  lr: 0.0011988  max_mem: 1773M
[32m[10/25 21:28:49 d2.utils.events]: [0m eta: 0:34:10  iter: 59  total_loss: 2.261  loss_cls: 0.3072  loss_box_reg: 0.3484  loss_mask: 0.5444  loss_rpn_cls: 0.5341  loss_rpn_loc: 0.5399  time: 0.1406  data_time: 0.0149  lr: 0.0017982  max_mem: 1773M
[32m[10/25 21:28:52 d2.utils.events]: [0m eta: 0:34:48  iter: 79  total_loss: 2.123  loss_cls: 0.2705  loss_box_reg: 0.4006  loss_mask: 0.5249  loss_rpn_cls: 0.3998  loss_rpn_loc: 0.5227  time: 0.1416  data_time: 0.0156  lr: 0.0023976  max_mem: 1773M
[32m[10/25 21:28:55 d2.utils.events]: [0m eta: 0:35:19  iter: 99  total_loss: 1.944  loss_cls: 0.2765  loss_box_reg: 0.4047  loss_mask: 0.4896  loss_rpn_cls: 0.3495  loss_rpn_loc: 0.5431  time: 0.1425  data_time: 0.0158  lr: 0.002997  max_mem: 1798M
[32m[10/25 21:28:58 d2.utils.events]: [0m eta: 0:35:30  iter: 119  total_loss: 1.837  loss_cls: 0.2689  loss_box_reg: 0.3624  loss_mask: 0.4364  loss_rpn_cls: 0.299  loss_rpn_loc: 0.5103  time: 0.1428  data_time: 0.0162  lr: 0.0035964  max_mem: 1798M
[32m[10/25 21:29:01 d2.utils.events]: [0m eta: 0:35:46  iter: 139  total_loss: 1.852  loss_cls: 0.2476  loss_box_reg: 0.4062  loss_mask: 0.3996  loss_rpn_cls: 0.2862  loss_rpn_loc: 0.5397  time: 0.1431  data_time: 0.0151  lr: 0.0041958  max_mem: 1798M
[32m[10/25 21:29:04 d2.utils.events]: [0m eta: 0:35:39  iter: 159  total_loss: 2.089  loss_cls: 0.2896  loss_box_reg: 0.4786  loss_mask: 0.3956  loss_rpn_cls: 0.3363  loss_rpn_loc: 0.5367  time: 0.1430  data_time: 0.0160  lr: 0.0047952  max_mem: 1798M
[32m[10/25 21:29:06 d2.utils.events]: [0m eta: 0:35:28  iter: 179  total_loss: 1.694  loss_cls: 0.2233  loss_box_reg: 0.4118  loss_mask: 0.355  loss_rpn_cls: 0.2626  loss_rpn_loc: 0.496  time: 0.1426  data_time: 0.0156  lr: 0.0053946  max_mem: 1798M
[32m[10/25 21:29:09 d2.utils.events]: [0m eta: 0:35:23  iter: 199  total_loss: 1.734  loss_cls: 0.221  loss_box_reg: 0.3505  loss_mask: 0.365  loss_rpn_cls: 0.2419  loss_rpn_loc: 0.5209  time: 0.1426  data_time: 0.0141  lr: 0.005994  max_mem: 1822M
[32m[10/25 21:29:12 d2.utils.events]: [0m eta: 0:35:20  iter: 219  total_loss: 1.808  loss_cls: 0.2761  loss_box_reg: 0.4549  loss_mask: 0.3239  loss_rpn_cls: 0.2514  loss_rpn_loc: 0.4683  time: 0.1427  data_time: 0.0164  lr: 0.0065934  max_mem: 1822M
[32m[10/25 21:29:15 d2.utils.events]: [0m eta: 0:35:23  iter: 239  total_loss: 1.661  loss_cls: 0.2269  loss_box_reg: 0.3981  loss_mask: 0.3145  loss_rpn_cls: 0.2663  loss_rpn_loc: 0.4372  time: 0.1430  data_time: 0.0163  lr: 0.0071928  max_mem: 1822M
[32m[10/25 21:29:18 d2.utils.events]: [0m eta: 0:35:21  iter: 259  total_loss: 1.656  loss_cls: 0.2237  loss_box_reg: 0.4053  loss_mask: 0.3254  loss_rpn_cls: 0.2773  loss_rpn_loc: 0.4637  time: 0.1435  data_time: 0.0145  lr: 0.0077922  max_mem: 1822M
[32m[10/25 21:29:22 d2.utils.events]: [0m eta: 0:35:21  iter: 279  total_loss: 1.566  loss_cls: 0.1841  loss_box_reg: 0.3467  loss_mask: 0.3073  loss_rpn_cls: 0.2295  loss_rpn_loc: 0.4654  time: 0.1440  data_time: 0.0158  lr: 0.0083916  max_mem: 1822M
[32m[10/25 21:29:24 d2.utils.events]: [0m eta: 0:35:15  iter: 299  total_loss: 1.621  loss_cls: 0.2129  loss_box_reg: 0.4016  loss_mask: 0.3035  loss_rpn_cls: 0.2521  loss_rpn_loc: 0.4742  time: 0.1438  data_time: 0.0152  lr: 0.008991  max_mem: 1822M
[32m[10/25 21:29:27 d2.utils.events]: [0m eta: 0:35:20  iter: 319  total_loss: 1.434  loss_cls: 0.1862  loss_box_reg: 0.3911  loss_mask: 0.2843  loss_rpn_cls: 0.1983  loss_rpn_loc: 0.4231  time: 0.1444  data_time: 0.0159  lr: 0.0095904  max_mem: 1822M
[32m[10/25 21:29:30 d2.utils.events]: [0m eta: 0:35:13  iter: 339  total_loss: 1.626  loss_cls: 0.2092  loss_box_reg: 0.3755  loss_mask: 0.2909  loss_rpn_cls: 0.2105  loss_rpn_loc: 0.4937  time: 0.1444  data_time: 0.0155  lr: 0.01019  max_mem: 1822M
[32m[10/25 21:29:33 d2.utils.events]: [0m eta: 0:35:13  iter: 359  total_loss: 1.622  loss_cls: 0.1911  loss_box_reg: 0.3263  loss_mask: 0.301  loss_rpn_cls: 0.2697  loss_rpn_loc: 0.4715  time: 0.1446  data_time: 0.0163  lr: 0.010789  max_mem: 1822M
[32m[10/25 21:29:36 d2.utils.events]: [0m eta: 0:35:10  iter: 379  total_loss: 1.51  loss_cls: 0.1477  loss_box_reg: 0.1744  loss_mask: 0.2893  loss_rpn_cls: 0.346  loss_rpn_loc: 0.54  time: 0.1444  data_time: 0.0152  lr: 0.011389  max_mem: 1822M
[32m[10/25 21:29:39 d2.utils.events]: [0m eta: 0:35:01  iter: 399  total_loss: 1.496  loss_cls: 0.1521  loss_box_reg: 0.2487  loss_mask: 0.2765  loss_rpn_cls: 0.2385  loss_rpn_loc: 0.5755  time: 0.1443  data_time: 0.0156  lr: 0.011988  max_mem: 1822M
[32m[10/25 21:29:42 d2.utils.events]: [0m eta: 0:34:57  iter: 419  total_loss: 1.498  loss_cls: 0.1803  loss_box_reg: 0.298  loss_mask: 0.276  loss_rpn_cls: 0.2187  loss_rpn_loc: 0.4856  time: 0.1441  data_time: 0.0151  lr: 0.012587  max_mem: 1822M
[32m[10/25 21:29:45 d2.utils.events]: [0m eta: 0:34:55  iter: 439  total_loss: 1.491  loss_cls: 0.1812  loss_box_reg: 0.3587  loss_mask: 0.2605  loss_rpn_cls: 0.2384  loss_rpn_loc: 0.4774  time: 0.1441  data_time: 0.0156  lr: 0.013187  max_mem: 1822M
[32m[10/25 21:29:48 d2.utils.events]: [0m eta: 0:34:56  iter: 459  total_loss: 1.586  loss_cls: 0.2027  loss_box_reg: 0.3556  loss_mask: 0.2589  loss_rpn_cls: 0.2418  loss_rpn_loc: 0.505  time: 0.1443  data_time: 0.0167  lr: 0.013786  max_mem: 1822M
[32m[10/25 21:29:51 d2.utils.events]: [0m eta: 0:34:56  iter: 479  total_loss: 1.653  loss_cls: 0.2479  loss_box_reg: 0.3958  loss_mask: 0.261  loss_rpn_cls: 0.2603  loss_rpn_loc: 0.4722  time: 0.1443  data_time: 0.0151  lr: 0.014386  max_mem: 1822M
[32m[10/25 21:29:54 d2.utils.events]: [0m eta: 0:34:53  iter: 499  total_loss: 1.486  loss_cls: 0.1969  loss_box_reg: 0.3947  loss_mask: 0.2504  loss_rpn_cls: 0.2253  loss_rpn_loc: 0.4753  time: 0.1443  data_time: 0.0151  lr: 0.014985  max_mem: 1822M
[32m[10/25 21:29:56 d2.utils.events]: [0m eta: 0:34:48  iter: 519  total_loss: 1.433  loss_cls: 0.1893  loss_box_reg: 0.2975  loss_mask: 0.2582  loss_rpn_cls: 0.1861  loss_rpn_loc: 0.4859  time: 0.1442  data_time: 0.0152  lr: 0.015584  max_mem: 1822M
[32m[10/25 21:29:59 d2.utils.events]: [0m eta: 0:34:43  iter: 539  total_loss: 1.516  loss_cls: 0.1874  loss_box_reg: 0.3381  loss_mask: 0.2538  loss_rpn_cls: 0.2183  loss_rpn_loc: 0.5299  time: 0.1442  data_time: 0.0156  lr: 0.016184  max_mem: 1822M
[32m[10/25 21:30:03 d2.utils.events]: [0m eta: 0:34:42  iter: 559  total_loss: 1.282  loss_cls: 0.1381  loss_box_reg: 0.2325  loss_mask: 0.2483  loss_rpn_cls: 0.2157  loss_rpn_loc: 0.4883  time: 0.1443  data_time: 0.0156  lr: 0.016783  max_mem: 1822M
[32m[10/25 21:30:05 d2.utils.events]: [0m eta: 0:34:36  iter: 579  total_loss: 1.421  loss_cls: 0.1937  loss_box_reg: 0.306  loss_mask: 0.2464  loss_rpn_cls: 0.1803  loss_rpn_loc: 0.4906  time: 0.1442  data_time: 0.0156  lr: 0.017383  max_mem: 1822M
[32m[10/25 21:30:08 d2.utils.events]: [0m eta: 0:34:38  iter: 599  total_loss: 1.488  loss_cls: 0.2123  loss_box_reg: 0.3896  loss_mask: 0.2424  loss_rpn_cls: 0.1834  loss_rpn_loc: 0.4399  time: 0.1443  data_time: 0.0157  lr: 0.017982  max_mem: 1822M
[32m[10/25 21:30:11 d2.utils.events]: [0m eta: 0:34:33  iter: 619  total_loss: 1.558  loss_cls: 0.226  loss_box_reg: 0.3813  loss_mask: 0.2435  loss_rpn_cls: 0.2093  loss_rpn_loc: 0.4386  time: 0.1443  data_time: 0.0149  lr: 0.018581  max_mem: 1822M
[32m[10/25 21:30:14 d2.utils.events]: [0m eta: 0:34:32  iter: 639  total_loss: 1.598  loss_cls: 0.2351  loss_box_reg: 0.3905  loss_mask: 0.2439  loss_rpn_cls: 0.1887  loss_rpn_loc: 0.4971  time: 0.1444  data_time: 0.0157  lr: 0.019181  max_mem: 1822M
[32m[10/25 21:30:17 d2.utils.events]: [0m eta: 0:34:31  iter: 659  total_loss: 1.64  loss_cls: 0.2529  loss_box_reg: 0.4699  loss_mask: 0.2512  loss_rpn_cls: 0.1974  loss_rpn_loc: 0.4349  time: 0.1445  data_time: 0.0156  lr: 0.01978  max_mem: 1822M
[32m[10/25 21:30:20 d2.utils.events]: [0m eta: 0:34:28  iter: 679  total_loss: 1.429  loss_cls: 0.2131  loss_box_reg: 0.3599  loss_mask: 0.2343  loss_rpn_cls: 0.1769  loss_rpn_loc: 0.5151  time: 0.1445  data_time: 0.0163  lr: 0.02038  max_mem: 1822M
[32m[10/25 21:30:23 d2.utils.events]: [0m eta: 0:34:25  iter: 699  total_loss: 1.444  loss_cls: 0.1863  loss_box_reg: 0.2946  loss_mask: 0.2273  loss_rpn_cls: 0.2123  loss_rpn_loc: 0.4791  time: 0.1444  data_time: 0.0150  lr: 0.020979  max_mem: 1822M
[32m[10/25 21:30:26 d2.utils.events]: [0m eta: 0:34:22  iter: 719  total_loss: 1.484  loss_cls: 0.1856  loss_box_reg: 0.2978  loss_mask: 0.2601  loss_rpn_cls: 0.2086  loss_rpn_loc: 0.4966  time: 0.1445  data_time: 0.0151  lr: 0.021578  max_mem: 1822M
[32m[10/25 21:30:29 d2.utils.events]: [0m eta: 0:34:20  iter: 739  total_loss: 1.519  loss_cls: 0.2128  loss_box_reg: 0.3623  loss_mask: 0.2481  loss_rpn_cls: 0.2031  loss_rpn_loc: 0.4719  time: 0.1447  data_time: 0.0165  lr: 0.022178  max_mem: 1822M
[32m[10/25 21:30:32 d2.utils.events]: [0m eta: 0:34:17  iter: 759  total_loss: 1.495  loss_cls: 0.1938  loss_box_reg: 0.3326  loss_mask: 0.2397  loss_rpn_cls: 0.2014  loss_rpn_loc: 0.4916  time: 0.1446  data_time: 0.0148  lr: 0.022777  max_mem: 1822M
[32m[10/25 21:30:35 d2.utils.events]: [0m eta: 0:34:14  iter: 779  total_loss: 1.453  loss_cls: 0.2047  loss_box_reg: 0.3193  loss_mask: 0.2449  loss_rpn_cls: 0.2107  loss_rpn_loc: 0.4718  time: 0.1446  data_time: 0.0156  lr: 0.023377  max_mem: 1822M
[32m[10/25 21:30:38 d2.utils.events]: [0m eta: 0:34:11  iter: 799  total_loss: 1.471  loss_cls: 0.1926  loss_box_reg: 0.336  loss_mask: 0.2431  loss_rpn_cls: 0.2124  loss_rpn_loc: 0.4763  time: 0.1446  data_time: 0.0145  lr: 0.023976  max_mem: 1822M
[32m[10/25 21:30:41 d2.utils.events]: [0m eta: 0:34:11  iter: 819  total_loss: 1.529  loss_cls: 0.2342  loss_box_reg: 0.3883  loss_mask: 0.2438  loss_rpn_cls: 0.1944  loss_rpn_loc: 0.4595  time: 0.1447  data_time: 0.0162  lr: 0.024575  max_mem: 1822M
[32m[10/25 21:30:44 d2.utils.events]: [0m eta: 0:34:08  iter: 839  total_loss: 1.54  loss_cls: 0.2424  loss_box_reg: 0.3712  loss_mask: 0.2308  loss_rpn_cls: 0.1889  loss_rpn_loc: 0.4604  time: 0.1447  data_time: 0.0160  lr: 0.025175  max_mem: 1822M
[32m[10/25 21:30:47 d2.utils.events]: [0m eta: 0:34:04  iter: 859  total_loss: 1.422  loss_cls: 0.1576  loss_box_reg: 0.2414  loss_mask: 0.2436  loss_rpn_cls: 0.2476  loss_rpn_loc: 0.519  time: 0.1446  data_time: 0.0153  lr: 0.025774  max_mem: 1822M
[32m[10/25 21:30:49 d2.utils.events]: [0m eta: 0:34:01  iter: 879  total_loss: 1.453  loss_cls: 0.142  loss_box_reg: 0.2392  loss_mask: 0.2383  loss_rpn_cls: 0.2729  loss_rpn_loc: 0.5645  time: 0.1446  data_time: 0.0154  lr: 0.026374  max_mem: 1822M
[32m[10/25 21:30:52 d2.utils.events]: [0m eta: 0:33:59  iter: 899  total_loss: 1.416  loss_cls: 0.1965  loss_box_reg: 0.2988  loss_mask: 0.233  loss_rpn_cls: 0.1745  loss_rpn_loc: 0.474  time: 0.1446  data_time: 0.0158  lr: 0.026973  max_mem: 1822M
[32m[10/25 21:30:55 d2.utils.events]: [0m eta: 0:33:55  iter: 919  total_loss: 1.451  loss_cls: 0.1875  loss_box_reg: 0.309  loss_mask: 0.215  loss_rpn_cls: 0.1996  loss_rpn_loc: 0.4815  time: 0.1446  data_time: 0.0147  lr: 0.027572  max_mem: 1822M
[32m[10/25 21:30:58 d2.utils.events]: [0m eta: 0:33:52  iter: 939  total_loss: 1.41  loss_cls: 0.1864  loss_box_reg: 0.3091  loss_mask: 0.2273  loss_rpn_cls: 0.1972  loss_rpn_loc: 0.5081  time: 0.1446  data_time: 0.0159  lr: 0.028172  max_mem: 1822M
[32m[10/25 21:31:01 d2.utils.events]: [0m eta: 0:33:51  iter: 959  total_loss: 1.469  loss_cls: 0.1997  loss_box_reg: 0.3095  loss_mask: 0.2185  loss_rpn_cls: 0.2242  loss_rpn_loc: 0.5151  time: 0.1446  data_time: 0.0161  lr: 0.028771  max_mem: 1822M
[32m[10/25 21:31:04 d2.utils.events]: [0m eta: 0:33:47  iter: 979  total_loss: 1.402  loss_cls: 0.1719  loss_box_reg: 0.282  loss_mask: 0.2189  loss_rpn_cls: 0.2134  loss_rpn_loc: 0.5049  time: 0.1446  data_time: 0.0149  lr: 0.029371  max_mem: 1822M
[32m[10/25 21:31:08 d2.utils.events]: [0m eta: 0:33:43  iter: 999  total_loss: 1.411  loss_cls: 0.1422  loss_box_reg: 0.226  loss_mask: 0.2318  loss_rpn_cls: 0.2554  loss_rpn_loc: 0.5516  time: 0.1445  data_time: 0.0165  lr: 0.02997  max_mem: 1822M
[32m[10/25 21:31:10 d2.utils.events]: [0m eta: 0:33:43  iter: 1019  total_loss: 1.331  loss_cls: 0.1591  loss_box_reg: 0.2457  loss_mask: 0.2253  loss_rpn_cls: 0.1993  loss_rpn_loc: 0.4847  time: 0.1445  data_time: 0.0151  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:13 d2.utils.events]: [0m eta: 0:33:40  iter: 1039  total_loss: 1.423  loss_cls: 0.1948  loss_box_reg: 0.2944  loss_mask: 0.2212  loss_rpn_cls: 0.1911  loss_rpn_loc: 0.4665  time: 0.1445  data_time: 0.0154  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:16 d2.utils.events]: [0m eta: 0:33:38  iter: 1059  total_loss: 1.488  loss_cls: 0.165  loss_box_reg: 0.2826  loss_mask: 0.2301  loss_rpn_cls: 0.2377  loss_rpn_loc: 0.4986  time: 0.1445  data_time: 0.0152  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:19 d2.utils.events]: [0m eta: 0:33:34  iter: 1079  total_loss: 1.366  loss_cls: 0.169  loss_box_reg: 0.2097  loss_mask: 0.2159  loss_rpn_cls: 0.251  loss_rpn_loc: 0.4963  time: 0.1445  data_time: 0.0162  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:22 d2.utils.events]: [0m eta: 0:33:29  iter: 1099  total_loss: 1.326  loss_cls: 0.157  loss_box_reg: 0.21  loss_mask: 0.2228  loss_rpn_cls: 0.2033  loss_rpn_loc: 0.5171  time: 0.1445  data_time: 0.0156  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:25 d2.utils.events]: [0m eta: 0:33:25  iter: 1119  total_loss: 1.321  loss_cls: 0.1388  loss_box_reg: 0.2134  loss_mask: 0.2294  loss_rpn_cls: 0.1909  loss_rpn_loc: 0.5254  time: 0.1445  data_time: 0.0157  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:28 d2.utils.events]: [0m eta: 0:33:20  iter: 1139  total_loss: 1.353  loss_cls: 0.1452  loss_box_reg: 0.2286  loss_mask: 0.2233  loss_rpn_cls: 0.2736  loss_rpn_loc: 0.5064  time: 0.1445  data_time: 0.0157  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:31 d2.utils.events]: [0m eta: 0:33:19  iter: 1159  total_loss: 1.489  loss_cls: 0.1938  loss_box_reg: 0.376  loss_mask: 0.2168  loss_rpn_cls: 0.2782  loss_rpn_loc: 0.4828  time: 0.1445  data_time: 0.0158  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:34 d2.utils.events]: [0m eta: 0:33:16  iter: 1179  total_loss: 1.303  loss_cls: 0.1346  loss_box_reg: 0.2184  loss_mask: 0.2268  loss_rpn_cls: 0.2084  loss_rpn_loc: 0.5115  time: 0.1444  data_time: 0.0148  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:37 d2.utils.events]: [0m eta: 0:33:14  iter: 1199  total_loss: 1.288  loss_cls: 0.1109  loss_box_reg: 0.178  loss_mask: 0.2237  loss_rpn_cls: 0.241  loss_rpn_loc: 0.487  time: 0.1444  data_time: 0.0158  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:40 d2.utils.events]: [0m eta: 0:33:11  iter: 1219  total_loss: 1.349  loss_cls: 0.1466  loss_box_reg: 0.2283  loss_mask: 0.2307  loss_rpn_cls: 0.1937  loss_rpn_loc: 0.512  time: 0.1444  data_time: 0.0154  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:42 d2.utils.events]: [0m eta: 0:33:07  iter: 1239  total_loss: 1.373  loss_cls: 0.1438  loss_box_reg: 0.2129  loss_mask: 0.2181  loss_rpn_cls: 0.2273  loss_rpn_loc: 0.5091  time: 0.1443  data_time: 0.0155  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:45 d2.utils.events]: [0m eta: 0:33:03  iter: 1259  total_loss: 1.415  loss_cls: 0.1675  loss_box_reg: 0.2959  loss_mask: 0.2221  loss_rpn_cls: 0.2305  loss_rpn_loc: 0.5047  time: 0.1443  data_time: 0.0150  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:48 d2.utils.events]: [0m eta: 0:33:00  iter: 1279  total_loss: 1.299  loss_cls: 0.1334  loss_box_reg: 0.1983  loss_mask: 0.2261  loss_rpn_cls: 0.2236  loss_rpn_loc: 0.5223  time: 0.1443  data_time: 0.0166  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:51 d2.utils.events]: [0m eta: 0:32:57  iter: 1299  total_loss: 1.307  loss_cls: 0.1069  loss_box_reg: 0.1901  loss_mask: 0.221  loss_rpn_cls: 0.2335  loss_rpn_loc: 0.5644  time: 0.1443  data_time: 0.0152  lr: 0.03  max_mem: 1822M
[32m[10/25 21:31:54 d2.utils.events]: [0m eta: 0:32:51  iter: 1319  total_loss: 1.393  loss_cls: 0.08156  loss_box_reg: 0.1206  loss_mask: 0.2217  loss_rpn_cls: 0.4376  loss_rpn_loc: 0.5378  time: 0.1442  data_time: 0.0162  lr: 0.03  max_mem: 1822M
[4m[5m[31mERROR[0m [32m[10/25 21:31:57 d2.engine.train_loop]: [0mException during training:
Traceback (most recent call last):
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 138, in train
    self.run_step()
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 441, in run_step
    self._trainer.run_step()
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 242, in run_step
    self._write_metrics(loss_dict, data_time)
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 284, in _write_metrics
    raise FloatingPointError(
FloatingPointError: Loss became infinite or NaN at iteration=1338!
loss_dict = {'loss_cls': nan, 'loss_box_reg': nan, 'loss_mask': 0.7341752648353577, 'loss_rpn_cls': 0.566049337387085, 'loss_rpn_loc': 0.5450785160064697}
[32m[10/25 21:31:57 d2.engine.hooks]: [0mOverall training speed: 1336 iterations in 0:03:12 (0.1442 s / it)
[32m[10/25 21:31:57 d2.engine.hooks]: [0mTotal training time: 0:03:15 (0:00:03 on hooks)
[32m[10/25 21:31:57 d2.utils.events]: [0m eta: 0:32:46  iter: 1338  total_loss: 1.717  loss_cls: 0.1768  loss_box_reg: 0.1065  loss_mask: 0.2483  loss_rpn_cls: 0.5116  loss_rpn_loc: 0.7543  time: 0.1441  data_time: 0.0167  lr: 0.03  max_mem: 1822M
Traceback (most recent call last):
  File "train_powder_LR-0_03_T2.py", line 126, in <module>
    trainer.train()  # train the model!
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 431, in train
    super().train(self.start_iter, self.max_iter)
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 138, in train
    self.run_step()
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 441, in run_step
    self._trainer.run_step()
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 242, in run_step
    self._write_metrics(loss_dict, data_time)
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 284, in _write_metrics
    raise FloatingPointError(
FloatingPointError: Loss became infinite or NaN at iteration=1338!
loss_dict = {'loss_cls': nan, 'loss_box_reg': nan, 'loss_mask': 0.7341752648353577, 'loss_rpn_cls': 0.566049337387085, 'loss_rpn_loc': 0.5450785160064697}
/var/spool/slurm/d/job4743702/slurm_script: line 9: ./gpua.out: No such file or directory
