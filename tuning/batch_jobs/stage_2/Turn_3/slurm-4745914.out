Registered Datasets: ['satellite_Train', 'satellite_Val']
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/HP743_11S_500x.png
	num_instances: 25
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S04_03_SE1_1000X53.png
	num_instances: 40
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S03_01_SE1_1250X28.png
	num_instances: 123
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S08_03_SE1_1000X03.png
	num_instances: 88
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S06_01_SE1_500X67.png
	num_instances: 54
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S04_02_SE1_1000X50.png
	num_instances: 45
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S03_03_SE1_1250X41.png
	num_instances: 111
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/S02_01_SE1_300X14.png
	num_instances: 98
ddict info:
	path: ../../SALAS_Rep/../../../../../../../../ocean/projects/dmr200021p/sprice/initial_paper_complete_set/HP743_2S_250x.png
	num_instances: 49
Weights not found, weights will be downloaded from source: https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl
[32m[10/26 11:20:14 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[32m[10/26 11:20:15 d2.data.build]: [0mRemoved 0 images with no usable annotations. 20 images left.
[32m[10/26 11:20:15 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
| satellite  | 1412         |
|            |              |[0m
[32m[10/26 11:20:15 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[10/26 11:20:15 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[10/26 11:20:15 d2.data.common]: [0mSerializing 20 elements to byte tensors and concatenating them all ...
[32m[10/26 11:20:15 d2.data.common]: [0mSerialized dataset takes 0.46 MiB
[5m[31mWARNING[0m [32m[10/26 11:20:15 d2.solver.build]: [0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[32m[10/26 11:20:15 d2.checkpoint.c2_model_loading]: [0mRenaming Caffe2 weights ......
[32m[10/26 11:20:15 d2.checkpoint.c2_model_loading]: [0mFollowing weights matched with submodule backbone.bottom_up:
| Names in Model    | Names in Checkpoint      | Shapes                                          |
|:------------------|:-------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*           | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                  | (64, 3, 7, 7)                                   |
[32m[10/26 11:20:15 d2.engine.train_loop]: [0mStarting training from iteration 0
/jet/home/sprice/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[32m[10/26 11:20:20 d2.utils.events]: [0m eta: 0:33:36  iter: 19  total_loss: 2.789  loss_cls: 0.487  loss_box_reg: 0.03917  loss_mask: 0.6342  loss_rpn_cls: 0.6579  loss_rpn_loc: 0.9677  time: 0.1403  data_time: 0.1027  lr: 0.00059943  max_mem: 1700M
[32m[10/26 11:20:23 d2.utils.events]: [0m eta: 0:33:52  iter: 39  total_loss: 2.212  loss_cls: 0.3136  loss_box_reg: 0.1029  loss_mask: 0.5937  loss_rpn_cls: 0.5852  loss_rpn_loc: 0.6355  time: 0.1376  data_time: 0.0145  lr: 0.0011988  max_mem: 1800M
[32m[10/26 11:20:25 d2.utils.events]: [0m eta: 0:33:50  iter: 59  total_loss: 2.182  loss_cls: 0.2604  loss_box_reg: 0.2517  loss_mask: 0.5797  loss_rpn_cls: 0.5599  loss_rpn_loc: 0.5146  time: 0.1362  data_time: 0.0139  lr: 0.0017982  max_mem: 1800M
[32m[10/26 11:20:28 d2.utils.events]: [0m eta: 0:33:48  iter: 79  total_loss: 2.147  loss_cls: 0.299  loss_box_reg: 0.437  loss_mask: 0.5357  loss_rpn_cls: 0.392  loss_rpn_loc: 0.5145  time: 0.1366  data_time: 0.0148  lr: 0.0023976  max_mem: 1800M
[32m[10/26 11:20:31 d2.utils.events]: [0m eta: 0:33:54  iter: 99  total_loss: 2.036  loss_cls: 0.2802  loss_box_reg: 0.3513  loss_mask: 0.505  loss_rpn_cls: 0.3208  loss_rpn_loc: 0.5225  time: 0.1372  data_time: 0.0158  lr: 0.002997  max_mem: 1825M
[32m[10/26 11:20:34 d2.utils.events]: [0m eta: 0:33:54  iter: 119  total_loss: 1.926  loss_cls: 0.2731  loss_box_reg: 0.4342  loss_mask: 0.4639  loss_rpn_cls: 0.2612  loss_rpn_loc: 0.5048  time: 0.1371  data_time: 0.0139  lr: 0.0035964  max_mem: 1825M
[32m[10/26 11:20:36 d2.utils.events]: [0m eta: 0:33:54  iter: 139  total_loss: 1.826  loss_cls: 0.2219  loss_box_reg: 0.3285  loss_mask: 0.452  loss_rpn_cls: 0.2951  loss_rpn_loc: 0.5238  time: 0.1375  data_time: 0.0157  lr: 0.0041958  max_mem: 1825M
[32m[10/26 11:20:39 d2.utils.events]: [0m eta: 0:33:58  iter: 159  total_loss: 1.849  loss_cls: 0.255  loss_box_reg: 0.4174  loss_mask: 0.3798  loss_rpn_cls: 0.296  loss_rpn_loc: 0.4988  time: 0.1381  data_time: 0.0152  lr: 0.0047952  max_mem: 1825M
[32m[10/26 11:20:42 d2.utils.events]: [0m eta: 0:33:55  iter: 179  total_loss: 1.737  loss_cls: 0.2552  loss_box_reg: 0.4196  loss_mask: 0.3747  loss_rpn_cls: 0.3008  loss_rpn_loc: 0.5348  time: 0.1382  data_time: 0.0139  lr: 0.0053946  max_mem: 1825M
[32m[10/26 11:20:45 d2.utils.events]: [0m eta: 0:34:01  iter: 199  total_loss: 1.807  loss_cls: 0.2471  loss_box_reg: 0.3996  loss_mask: 0.3557  loss_rpn_cls: 0.2324  loss_rpn_loc: 0.4993  time: 0.1385  data_time: 0.0143  lr: 0.005994  max_mem: 1825M
[32m[10/26 11:20:48 d2.utils.events]: [0m eta: 0:34:07  iter: 219  total_loss: 1.666  loss_cls: 0.2053  loss_box_reg: 0.4226  loss_mask: 0.3243  loss_rpn_cls: 0.2523  loss_rpn_loc: 0.458  time: 0.1386  data_time: 0.0145  lr: 0.0065934  max_mem: 1825M
[32m[10/26 11:20:51 d2.utils.events]: [0m eta: 0:34:02  iter: 239  total_loss: 1.775  loss_cls: 0.2689  loss_box_reg: 0.392  loss_mask: 0.3302  loss_rpn_cls: 0.2625  loss_rpn_loc: 0.5243  time: 0.1386  data_time: 0.0150  lr: 0.0071928  max_mem: 1825M
[32m[10/26 11:20:54 d2.utils.events]: [0m eta: 0:34:04  iter: 259  total_loss: 1.803  loss_cls: 0.2666  loss_box_reg: 0.4746  loss_mask: 0.3052  loss_rpn_cls: 0.2209  loss_rpn_loc: 0.4818  time: 0.1390  data_time: 0.0146  lr: 0.0077922  max_mem: 1825M
[32m[10/26 11:20:56 d2.utils.events]: [0m eta: 0:34:03  iter: 279  total_loss: 1.726  loss_cls: 0.2603  loss_box_reg: 0.4332  loss_mask: 0.2978  loss_rpn_cls: 0.2688  loss_rpn_loc: 0.4801  time: 0.1391  data_time: 0.0151  lr: 0.0083916  max_mem: 1825M
[32m[10/26 11:20:59 d2.utils.events]: [0m eta: 0:34:06  iter: 299  total_loss: 1.776  loss_cls: 0.2808  loss_box_reg: 0.4661  loss_mask: 0.295  loss_rpn_cls: 0.2571  loss_rpn_loc: 0.4583  time: 0.1397  data_time: 0.0144  lr: 0.008991  max_mem: 1825M
[32m[10/26 11:21:02 d2.utils.events]: [0m eta: 0:34:05  iter: 319  total_loss: 1.699  loss_cls: 0.2563  loss_box_reg: 0.4335  loss_mask: 0.3133  loss_rpn_cls: 0.2064  loss_rpn_loc: 0.4366  time: 0.1407  data_time: 0.0188  lr: 0.0095904  max_mem: 1825M
[32m[10/26 11:21:05 d2.utils.events]: [0m eta: 0:34:06  iter: 339  total_loss: 1.781  loss_cls: 0.2533  loss_box_reg: 0.5015  loss_mask: 0.3023  loss_rpn_cls: 0.1993  loss_rpn_loc: 0.4731  time: 0.1413  data_time: 0.0161  lr: 0.01019  max_mem: 1825M
[32m[10/26 11:21:08 d2.utils.events]: [0m eta: 0:34:10  iter: 359  total_loss: 1.676  loss_cls: 0.24  loss_box_reg: 0.4591  loss_mask: 0.2837  loss_rpn_cls: 0.2006  loss_rpn_loc: 0.5066  time: 0.1415  data_time: 0.0147  lr: 0.010789  max_mem: 1825M
[32m[10/26 11:21:11 d2.utils.events]: [0m eta: 0:34:17  iter: 379  total_loss: 1.619  loss_cls: 0.2212  loss_box_reg: 0.3384  loss_mask: 0.2807  loss_rpn_cls: 0.2532  loss_rpn_loc: 0.4986  time: 0.1415  data_time: 0.0150  lr: 0.011389  max_mem: 1825M
[32m[10/26 11:21:14 d2.utils.events]: [0m eta: 0:34:13  iter: 399  total_loss: 1.638  loss_cls: 0.2109  loss_box_reg: 0.3618  loss_mask: 0.2688  loss_rpn_cls: 0.2737  loss_rpn_loc: 0.5277  time: 0.1414  data_time: 0.0143  lr: 0.011988  max_mem: 1825M
[32m[10/26 11:21:17 d2.utils.events]: [0m eta: 0:34:09  iter: 419  total_loss: 1.546  loss_cls: 0.2171  loss_box_reg: 0.3886  loss_mask: 0.2634  loss_rpn_cls: 0.2169  loss_rpn_loc: 0.52  time: 0.1413  data_time: 0.0149  lr: 0.012587  max_mem: 1825M
[32m[10/26 11:21:20 d2.utils.events]: [0m eta: 0:34:09  iter: 439  total_loss: 1.557  loss_cls: 0.2321  loss_box_reg: 0.4029  loss_mask: 0.2705  loss_rpn_cls: 0.2158  loss_rpn_loc: 0.4417  time: 0.1415  data_time: 0.0155  lr: 0.013187  max_mem: 1825M
[32m[10/26 11:21:23 d2.utils.events]: [0m eta: 0:34:08  iter: 459  total_loss: 1.508  loss_cls: 0.2131  loss_box_reg: 0.4045  loss_mask: 0.2602  loss_rpn_cls: 0.2195  loss_rpn_loc: 0.4453  time: 0.1416  data_time: 0.0159  lr: 0.013786  max_mem: 1825M
[32m[10/26 11:21:26 d2.utils.events]: [0m eta: 0:34:06  iter: 479  total_loss: 1.636  loss_cls: 0.2397  loss_box_reg: 0.4515  loss_mask: 0.2751  loss_rpn_cls: 0.2199  loss_rpn_loc: 0.472  time: 0.1415  data_time: 0.0142  lr: 0.014386  max_mem: 1825M
[32m[10/26 11:21:29 d2.utils.events]: [0m eta: 0:34:03  iter: 499  total_loss: 1.557  loss_cls: 0.1913  loss_box_reg: 0.3231  loss_mask: 0.268  loss_rpn_cls: 0.2384  loss_rpn_loc: 0.5479  time: 0.1415  data_time: 0.0153  lr: 0.014985  max_mem: 1825M
[32m[10/26 11:21:31 d2.utils.events]: [0m eta: 0:33:59  iter: 519  total_loss: 1.556  loss_cls: 0.1903  loss_box_reg: 0.3626  loss_mask: 0.2718  loss_rpn_cls: 0.2181  loss_rpn_loc: 0.5133  time: 0.1415  data_time: 0.0150  lr: 0.015584  max_mem: 1825M
[32m[10/26 11:21:34 d2.utils.events]: [0m eta: 0:33:56  iter: 539  total_loss: 1.48  loss_cls: 0.1866  loss_box_reg: 0.321  loss_mask: 0.2656  loss_rpn_cls: 0.2034  loss_rpn_loc: 0.5005  time: 0.1415  data_time: 0.0146  lr: 0.016184  max_mem: 1825M
[32m[10/26 11:21:37 d2.utils.events]: [0m eta: 0:33:53  iter: 559  total_loss: 1.523  loss_cls: 0.2108  loss_box_reg: 0.3654  loss_mask: 0.2746  loss_rpn_cls: 0.2332  loss_rpn_loc: 0.5055  time: 0.1414  data_time: 0.0147  lr: 0.016783  max_mem: 1825M
[32m[10/26 11:21:40 d2.utils.events]: [0m eta: 0:33:52  iter: 579  total_loss: 1.44  loss_cls: 0.1968  loss_box_reg: 0.388  loss_mask: 0.2441  loss_rpn_cls: 0.171  loss_rpn_loc: 0.4284  time: 0.1415  data_time: 0.0145  lr: 0.017383  max_mem: 1825M
[32m[10/26 11:21:43 d2.utils.events]: [0m eta: 0:33:48  iter: 599  total_loss: 1.433  loss_cls: 0.1935  loss_box_reg: 0.3098  loss_mask: 0.2643  loss_rpn_cls: 0.2012  loss_rpn_loc: 0.4557  time: 0.1414  data_time: 0.0150  lr: 0.017982  max_mem: 1825M
[32m[10/26 11:21:46 d2.utils.events]: [0m eta: 0:33:47  iter: 619  total_loss: 1.524  loss_cls: 0.2202  loss_box_reg: 0.3685  loss_mask: 0.2563  loss_rpn_cls: 0.2026  loss_rpn_loc: 0.4293  time: 0.1415  data_time: 0.0149  lr: 0.018581  max_mem: 1825M
[32m[10/26 11:21:48 d2.utils.events]: [0m eta: 0:33:43  iter: 639  total_loss: 1.446  loss_cls: 0.1643  loss_box_reg: 0.3694  loss_mask: 0.2477  loss_rpn_cls: 0.193  loss_rpn_loc: 0.5218  time: 0.1414  data_time: 0.0148  lr: 0.019181  max_mem: 1825M
[32m[10/26 11:21:51 d2.utils.events]: [0m eta: 0:33:42  iter: 659  total_loss: 1.504  loss_cls: 0.1965  loss_box_reg: 0.3356  loss_mask: 0.2425  loss_rpn_cls: 0.2138  loss_rpn_loc: 0.4911  time: 0.1415  data_time: 0.0150  lr: 0.01978  max_mem: 1825M
[32m[10/26 11:21:54 d2.utils.events]: [0m eta: 0:33:41  iter: 679  total_loss: 1.591  loss_cls: 0.216  loss_box_reg: 0.4002  loss_mask: 0.2415  loss_rpn_cls: 0.1969  loss_rpn_loc: 0.4956  time: 0.1416  data_time: 0.0153  lr: 0.02038  max_mem: 1825M
[32m[10/26 11:21:57 d2.utils.events]: [0m eta: 0:33:38  iter: 699  total_loss: 1.485  loss_cls: 0.2038  loss_box_reg: 0.3909  loss_mask: 0.2458  loss_rpn_cls: 0.1819  loss_rpn_loc: 0.4748  time: 0.1417  data_time: 0.0145  lr: 0.020979  max_mem: 1825M
[32m[10/26 11:22:00 d2.utils.events]: [0m eta: 0:33:37  iter: 719  total_loss: 1.502  loss_cls: 0.2237  loss_box_reg: 0.37  loss_mask: 0.2398  loss_rpn_cls: 0.2185  loss_rpn_loc: 0.4449  time: 0.1417  data_time: 0.0152  lr: 0.021578  max_mem: 1825M
[32m[10/26 11:22:03 d2.utils.events]: [0m eta: 0:33:33  iter: 739  total_loss: 1.467  loss_cls: 0.1843  loss_box_reg: 0.2959  loss_mask: 0.2395  loss_rpn_cls: 0.2254  loss_rpn_loc: 0.513  time: 0.1416  data_time: 0.0147  lr: 0.022178  max_mem: 1825M
[32m[10/26 11:22:06 d2.utils.events]: [0m eta: 0:33:28  iter: 759  total_loss: 1.516  loss_cls: 0.2121  loss_box_reg: 0.355  loss_mask: 0.239  loss_rpn_cls: 0.2044  loss_rpn_loc: 0.4876  time: 0.1416  data_time: 0.0146  lr: 0.022777  max_mem: 1825M
[32m[10/26 11:22:09 d2.utils.events]: [0m eta: 0:33:25  iter: 779  total_loss: 1.458  loss_cls: 0.1949  loss_box_reg: 0.3499  loss_mask: 0.25  loss_rpn_cls: 0.1885  loss_rpn_loc: 0.4657  time: 0.1416  data_time: 0.0149  lr: 0.023377  max_mem: 1825M
[32m[10/26 11:22:12 d2.utils.events]: [0m eta: 0:33:23  iter: 799  total_loss: 1.527  loss_cls: 0.2097  loss_box_reg: 0.4098  loss_mask: 0.2348  loss_rpn_cls: 0.2113  loss_rpn_loc: 0.4695  time: 0.1416  data_time: 0.0148  lr: 0.023976  max_mem: 1825M
[32m[10/26 11:22:14 d2.utils.events]: [0m eta: 0:33:23  iter: 819  total_loss: 1.542  loss_cls: 0.2502  loss_box_reg: 0.3815  loss_mask: 0.2394  loss_rpn_cls: 0.1743  loss_rpn_loc: 0.4495  time: 0.1417  data_time: 0.0154  lr: 0.024575  max_mem: 1825M
[32m[10/26 11:22:17 d2.utils.events]: [0m eta: 0:33:21  iter: 839  total_loss: 1.468  loss_cls: 0.199  loss_box_reg: 0.3553  loss_mask: 0.2356  loss_rpn_cls: 0.1949  loss_rpn_loc: 0.4737  time: 0.1417  data_time: 0.0154  lr: 0.025175  max_mem: 1825M
[32m[10/26 11:22:20 d2.utils.events]: [0m eta: 0:33:18  iter: 859  total_loss: 1.465  loss_cls: 0.1753  loss_box_reg: 0.3491  loss_mask: 0.257  loss_rpn_cls: 0.1955  loss_rpn_loc: 0.4757  time: 0.1417  data_time: 0.0146  lr: 0.025774  max_mem: 1825M
[32m[10/26 11:22:23 d2.utils.events]: [0m eta: 0:33:16  iter: 879  total_loss: 1.456  loss_cls: 0.1925  loss_box_reg: 0.3208  loss_mask: 0.2406  loss_rpn_cls: 0.1773  loss_rpn_loc: 0.4505  time: 0.1417  data_time: 0.0149  lr: 0.026374  max_mem: 1825M
[32m[10/26 11:22:26 d2.utils.events]: [0m eta: 0:33:13  iter: 899  total_loss: 1.441  loss_cls: 0.1875  loss_box_reg: 0.3199  loss_mask: 0.227  loss_rpn_cls: 0.2064  loss_rpn_loc: 0.4921  time: 0.1417  data_time: 0.0146  lr: 0.026973  max_mem: 1825M
[32m[10/26 11:22:29 d2.utils.events]: [0m eta: 0:33:10  iter: 919  total_loss: 1.586  loss_cls: 0.2226  loss_box_reg: 0.3645  loss_mask: 0.217  loss_rpn_cls: 0.2454  loss_rpn_loc: 0.505  time: 0.1417  data_time: 0.0148  lr: 0.027572  max_mem: 1825M
[32m[10/26 11:22:32 d2.utils.events]: [0m eta: 0:33:08  iter: 939  total_loss: 1.445  loss_cls: 0.1776  loss_box_reg: 0.3369  loss_mask: 0.2268  loss_rpn_cls: 0.2243  loss_rpn_loc: 0.4847  time: 0.1417  data_time: 0.0153  lr: 0.028172  max_mem: 1825M
[32m[10/26 11:22:34 d2.utils.events]: [0m eta: 0:33:05  iter: 959  total_loss: 1.437  loss_cls: 0.1321  loss_box_reg: 0.1995  loss_mask: 0.255  loss_rpn_cls: 0.2729  loss_rpn_loc: 0.5144  time: 0.1416  data_time: 0.0144  lr: 0.028771  max_mem: 1825M
[32m[10/26 11:22:37 d2.utils.events]: [0m eta: 0:33:02  iter: 979  total_loss: 1.496  loss_cls: 0.1801  loss_box_reg: 0.3014  loss_mask: 0.2372  loss_rpn_cls: 0.236  loss_rpn_loc: 0.4984  time: 0.1416  data_time: 0.0144  lr: 0.029371  max_mem: 1825M
[32m[10/26 11:22:41 d2.utils.events]: [0m eta: 0:32:58  iter: 999  total_loss: 1.559  loss_cls: 0.2148  loss_box_reg: 0.3469  loss_mask: 0.2368  loss_rpn_cls: 0.2304  loss_rpn_loc: 0.5151  time: 0.1416  data_time: 0.0149  lr: 0.02997  max_mem: 1825M
[32m[10/26 11:22:43 d2.utils.events]: [0m eta: 0:32:55  iter: 1019  total_loss: 1.547  loss_cls: 0.2031  loss_box_reg: 0.3506  loss_mask: 0.2367  loss_rpn_cls: 0.2402  loss_rpn_loc: 0.4902  time: 0.1415  data_time: 0.0147  lr: 0.03  max_mem: 1825M
[32m[10/26 11:22:46 d2.utils.events]: [0m eta: 0:32:53  iter: 1039  total_loss: 1.49  loss_cls: 0.1636  loss_box_reg: 0.3147  loss_mask: 0.2315  loss_rpn_cls: 0.2279  loss_rpn_loc: 0.5162  time: 0.1415  data_time: 0.0135  lr: 0.03  max_mem: 1825M
[32m[10/26 11:22:49 d2.utils.events]: [0m eta: 0:32:51  iter: 1059  total_loss: 1.474  loss_cls: 0.2121  loss_box_reg: 0.2981  loss_mask: 0.2254  loss_rpn_cls: 0.2239  loss_rpn_loc: 0.4694  time: 0.1415  data_time: 0.0148  lr: 0.03  max_mem: 1825M
[32m[10/26 11:22:52 d2.utils.events]: [0m eta: 0:32:48  iter: 1079  total_loss: 1.461  loss_cls: 0.1754  loss_box_reg: 0.3071  loss_mask: 0.2172  loss_rpn_cls: 0.2123  loss_rpn_loc: 0.5045  time: 0.1414  data_time: 0.0149  lr: 0.03  max_mem: 1825M
[32m[10/26 11:22:55 d2.utils.events]: [0m eta: 0:32:45  iter: 1099  total_loss: 1.388  loss_cls: 0.1758  loss_box_reg: 0.2836  loss_mask: 0.2255  loss_rpn_cls: 0.1977  loss_rpn_loc: 0.4779  time: 0.1414  data_time: 0.0151  lr: 0.03  max_mem: 1825M
[32m[10/26 11:22:58 d2.utils.events]: [0m eta: 0:32:44  iter: 1119  total_loss: 1.399  loss_cls: 0.166  loss_box_reg: 0.3159  loss_mask: 0.2173  loss_rpn_cls: 0.1991  loss_rpn_loc: 0.4872  time: 0.1414  data_time: 0.0145  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:00 d2.utils.events]: [0m eta: 0:32:40  iter: 1139  total_loss: 1.299  loss_cls: 0.1408  loss_box_reg: 0.2096  loss_mask: 0.2229  loss_rpn_cls: 0.223  loss_rpn_loc: 0.4998  time: 0.1413  data_time: 0.0144  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:03 d2.utils.events]: [0m eta: 0:32:38  iter: 1159  total_loss: 1.326  loss_cls: 0.1851  loss_box_reg: 0.2907  loss_mask: 0.2127  loss_rpn_cls: 0.1461  loss_rpn_loc: 0.4594  time: 0.1414  data_time: 0.0154  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:06 d2.utils.events]: [0m eta: 0:32:35  iter: 1179  total_loss: 1.32  loss_cls: 0.1374  loss_box_reg: 0.1812  loss_mask: 0.225  loss_rpn_cls: 0.2186  loss_rpn_loc: 0.521  time: 0.1413  data_time: 0.0153  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:09 d2.utils.events]: [0m eta: 0:32:31  iter: 1199  total_loss: 1.473  loss_cls: 0.1606  loss_box_reg: 0.2223  loss_mask: 0.2341  loss_rpn_cls: 0.264  loss_rpn_loc: 0.5933  time: 0.1412  data_time: 0.0146  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:12 d2.utils.events]: [0m eta: 0:32:28  iter: 1219  total_loss: 1.356  loss_cls: 0.1504  loss_box_reg: 0.2314  loss_mask: 0.2268  loss_rpn_cls: 0.273  loss_rpn_loc: 0.5366  time: 0.1412  data_time: 0.0151  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:15 d2.utils.events]: [0m eta: 0:32:25  iter: 1239  total_loss: 1.459  loss_cls: 0.1961  loss_box_reg: 0.2625  loss_mask: 0.2211  loss_rpn_cls: 0.2146  loss_rpn_loc: 0.5304  time: 0.1411  data_time: 0.0138  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:17 d2.utils.events]: [0m eta: 0:32:21  iter: 1259  total_loss: 1.374  loss_cls: 0.1556  loss_box_reg: 0.2811  loss_mask: 0.217  loss_rpn_cls: 0.1919  loss_rpn_loc: 0.4896  time: 0.1411  data_time: 0.0158  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:20 d2.utils.events]: [0m eta: 0:32:18  iter: 1279  total_loss: 1.436  loss_cls: 0.2018  loss_box_reg: 0.2959  loss_mask: 0.2247  loss_rpn_cls: 0.2272  loss_rpn_loc: 0.4955  time: 0.1411  data_time: 0.0145  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:23 d2.utils.events]: [0m eta: 0:32:16  iter: 1299  total_loss: 1.421  loss_cls: 0.1825  loss_box_reg: 0.3132  loss_mask: 0.2279  loss_rpn_cls: 0.1717  loss_rpn_loc: 0.485  time: 0.1411  data_time: 0.0156  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:26 d2.utils.events]: [0m eta: 0:32:13  iter: 1319  total_loss: 1.438  loss_cls: 0.2008  loss_box_reg: 0.3476  loss_mask: 0.2148  loss_rpn_cls: 0.159  loss_rpn_loc: 0.4719  time: 0.1411  data_time: 0.0148  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:29 d2.utils.events]: [0m eta: 0:32:08  iter: 1339  total_loss: 1.441  loss_cls: 0.1963  loss_box_reg: 0.3302  loss_mask: 0.2065  loss_rpn_cls: 0.1777  loss_rpn_loc: 0.4962  time: 0.1411  data_time: 0.0144  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:32 d2.utils.events]: [0m eta: 0:32:04  iter: 1359  total_loss: 1.394  loss_cls: 0.1766  loss_box_reg: 0.3167  loss_mask: 0.2029  loss_rpn_cls: 0.2024  loss_rpn_loc: 0.451  time: 0.1412  data_time: 0.0152  lr: 0.03  max_mem: 1825M
[32m[10/26 11:23:35 d2.utils.events]: [0m eta: 0:32:01  iter: 1379  total_loss: 1.352  loss_cls: 0.1602  loss_box_reg: 0.2715  loss_mask: 0.2212  loss_rpn_cls: 0.1728  loss_rpn_loc: 0.4961  time: 0.1412  data_time: 0.0150  lr: 0.03  max_mem: 1825M
[4m[5m[31mERROR[0m [32m[10/26 11:23:36 d2.engine.train_loop]: [0mException during training:
Traceback (most recent call last):
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 138, in train
    self.run_step()
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 441, in run_step
    self._trainer.run_step()
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 242, in run_step
    self._write_metrics(loss_dict, data_time)
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 284, in _write_metrics
    raise FloatingPointError(
FloatingPointError: Loss became infinite or NaN at iteration=1391!
loss_dict = {'loss_cls': nan, 'loss_box_reg': nan, 'loss_mask': 1.7929576412852926e+31, 'loss_rpn_cls': 0.6625514030456543, 'loss_rpn_loc': 0.6432416439056396}
[32m[10/26 11:23:36 d2.engine.hooks]: [0mOverall training speed: 1389 iterations in 0:03:16 (0.1413 s / it)
[32m[10/26 11:23:36 d2.engine.hooks]: [0mTotal training time: 0:03:19 (0:00:02 on hooks)
[32m[10/26 11:23:36 d2.utils.events]: [0m eta: 0:31:59  iter: 1391  total_loss: 1.36  loss_cls: 0.1558  loss_box_reg: 0.2429  loss_mask: 0.2499  loss_rpn_cls: 0.2436  loss_rpn_loc: 0.5257  time: 0.1411  data_time: 0.0152  lr: 0.03  max_mem: 1825M
Traceback (most recent call last):
  File "train_powder_LR-0_03_T3.py", line 126, in <module>
    trainer.train()  # train the model!
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 431, in train
    super().train(self.start_iter, self.max_iter)
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 138, in train
    self.run_step()
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 441, in run_step
    self._trainer.run_step()
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 242, in run_step
    self._write_metrics(loss_dict, data_time)
  File "/jet/home/sprice/.local/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 284, in _write_metrics
    raise FloatingPointError(
FloatingPointError: Loss became infinite or NaN at iteration=1391!
loss_dict = {'loss_cls': nan, 'loss_box_reg': nan, 'loss_mask': 1.7929576412852926e+31, 'loss_rpn_cls': 0.6625514030456543, 'loss_rpn_loc': 0.6432416439056396}
/var/spool/slurm/d/job4745914/slurm_script: line 9: ./gpua.out: No such file or directory
